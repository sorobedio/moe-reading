Reading


List of paper


| **Paper Title**                                                           | **Year** | **Link**                                                              | **Code**                                                             |
|----------------------------------------------------------------------------|----------|-----------------------------------------------------------------------|----------------------------------------------------------------------|
| SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS                           | 2024     | [Link](https://arxiv.org/abs/2402.07871)                              | [Code](https://github.com/llm-random/llm-random)                     |
| Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models | 2023     | [Link](https://arxiv.org/abs/2310.00456)                              |                                                                      |
| Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM          | 2023     | [Link](https://arxiv.org/abs/2307.05088)                              | [Code](https://github.com/branch-train-mix)                          |
| SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS        | 2023     | [Link](https://arxiv.org/abs/2306.05657)                              | [Code](https://github.com/google-research/sparse-upcycling)           |
| SELF-MOE: TOWARDS COMPOSITIONAL LARGE LANGUAGE MODELS WITH SELF-SPECIALIZED EXPERTS | 2023     | [Link](https://arxiv.org/abs/2302.01952)                              | [Code](https://github.com/self-moe/self-moe)                         |
| Upcycling Large Language Models into Mixture of Experts                    | 2023     | [Link](https://arxiv.org/abs/2301.08469)                              | [Code](https://github.com/google-research/upcycling-moe)              |
| Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling | 2023     | [Link](https://arxiv.org/abs/2301.07767)                              | [Code](https://github.com/EleutherAI/pythia)                         |
| Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts | 2022     | [Link](https://arxiv.org/abs/2203.07075)                              | [Code](https://github.com/facebookresearch/nexus)                    |

