Reading


List of paper



| **#** | **Paper Title**                                                           | **Year** | **Link**                                                              | **Code**                                                             |
|-------|----------------------------------------------------------------------------|----------|-----------------------------------------------------------------------|----------------------------------------------------------------------|
| 1     | SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS                           | 2024     | [arxiv](https://arxiv.org/abs/2402.07871)                              | [GitHub](https://github.com/llm-random/llm-random)                   |
| 2     | Upcycling Large Language Models into Mixture of Experts                    | 2024     | [arxiv](https://arxiv.org/abs/2410.07524)                              | No code available                                                    |
| 3     | Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts | 2024     | [arxiv](https://arxiv.org/abs/2408.15901)                              | No code available                                                    |
| 4     | SELF-MOE: TOWARDS COMPOSITIONAL LARGE LANGUAGE MODELS WITH SELF-SPECIALIZED EXPERTS | 2024     | [arxiv](https://arxiv.org/abs/2406.12034)                              | No code available                                                    |
| 5     | SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS        | 2023     | [arxiv](https://arxiv.org/abs/2212.05055)                              | [GitHub](https://github.com/google-research/vmoe)                    |
| 6     | Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM          | 2024     | [arxiv](https://arxiv.org/abs/2403.07816)                              | No code available                                                    |
| 7     | Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling | 2023     | [arxiv](https://arxiv.org/abs/2301.07767)                              | [GitHub](https://github.com/EleutherAI/pythia)                       |
| 8     | Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models | 2023     | [arxiv](https://arxiv.org/abs/2208.03306)                              | [GitHub](https://github.com/hadasah/btm)                             |


