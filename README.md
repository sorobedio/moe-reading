Reading


List of paper

SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS

Upcycling Large Language Models into Mixture of Experts

Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts

SELF-MOE: TOWARDS COMPOSITIONAL LARGE LANGUAGE MODELS WITH SELF-SPECIALIZED EXPERTS

Upcycling Large Language Models into Mixture of Experts

SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS

Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM

Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling
