Reading


# List of Mixture of Experts (MoE) and Large Language Model (LLM) Papers focusing on model Upcycling

This repository contains a collection of important papers related to Mixture of Experts (MoE) and Large Language Models (LLMs), along with links to their corresponding Arxiv pages and available GitHub code.


| **#** | **Paper Title**                                                                                                         | **Year** | **Link**                                                              | **Code**                                                             |
|-------|--------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------|----------------------------------------------------------------------|
| 1     | [Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence](https://arxiv.org/abs/2410.11163)       | 2024     | [Arxiv](https://arxiv.org/abs/2410.11163)                             | No code available                                                    |
| 2     | [Upcycling Large Language Models into Mixture of Experts](https://arxiv.org/abs/2410.07524)                              | 2024     | [Arxiv](https://arxiv.org/abs/2410.07524)                             | No code available                                                    |
| 3     | [Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts](https://arxiv.org/abs/2408.15901) | 2024     | [Arxiv](https://arxiv.org/abs/2408.15901)                             | No code available                                                    |
| 4     | [SELF-MOE: TOWARDS COMPOSITIONAL LARGE LANGUAGE MODELS WITH SELF-SPECIALIZED EXPERTS](https://arxiv.org/abs/2406.12034)  | 2024     | [Arxiv](https://arxiv.org/abs/2406.12034)                             | No code available                                                    |
| 5     | [Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization](https://arxiv.org/abs/2404.11531)                 | 2024     | [Arxiv](https://arxiv.org/abs/2404.11531)                             | [GitHub](https://github.com/cmavro/packllm)                          |
| 6     | [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816)                   | 2024     | [Arxiv](https://arxiv.org/abs/2403.07816)                             | No code available                                                    |
| 7     | [SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS](https://arxiv.org/abs/2402.07871)                                     | 2024     | [Arxiv](https://arxiv.org/abs/2402.07871)                             | [GitHub](https://github.com/llm-random/llm-random)                   |
| 8     | [Scaling expert language models with unsupervised domain discovery](https://arxiv.org/abs/2303.14177)                    | 2023     | [Arxiv](https://arxiv.org/abs/2303.14177)                             | [GitHub](https://github.com/kernelmachine/cbtm)                      |
| 9     | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)      | 2023     | [Arxiv](https://arxiv.org/abs/2304.01373)                             | [GitHub](https://github.com/eleutherai/pythia)                       |
| 10    | [Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models](https://arxiv.org/abs/2208.03306)       | 2023     | [Arxiv](https://arxiv.org/abs/2208.03306)                             | [GitHub](https://github.com/hadasah/btm)                             |
| 11    | [SPARSE UPCYCLING: TRAINING MIXTURE-OF-EXPERTS FROM DENSE CHECKPOINTS](https://arxiv.org/abs/2212.05055)                 | 2023     | [Arxiv](https://arxiv.org/abs/2212.05055)                             | [GitHub](https://github.com/google-research/vmoe)                    |
| 12   | [Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging](https://arxiv.org/abs/2410.01610)                 | 2024    | [Arxiv](https://arxiv.org/abs/2410.01610)                             | Not available                   |




***Advances in Weight Generation and Retrieval for Language Models**

| **#** | **Paper Title**                                                                                                         | **Year** | **Link**                                                              | **Code**                                                             |
|-------|--------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------|----------------------------------------------------------------------|
| 1     | [Representing Model Weights with Language using Tree Experts](https://arxiv.org/abs/2410.13569)                          | 2024     | [Arxiv](https://arxiv.org/abs/2410.13569)                             | No code available                                                    |
| 2     | [Deep Linear Probe Generators for Weight Space Learning](https://arxiv.org/abs/2410.10811)                               | 2024     | [Arxiv](https://arxiv.org/abs/2410.10811)                             | [GitHub](https://github.com/jonkahana/ProbeGen)                      |
| 3     | [Knowledge Fusion By Evolving Weights of Language Models](https://arxiv.org/abs/2406.12208)                              | 2024     | [Arxiv](https://arxiv.org/abs/2406.12208)                             | [GitHub](https://github.com/duguodong7/model-evolution)              |




Vector Quantization Prompting for Continual Learning

Historical Test-time Prompt Tuning for Vision Foundation Models




## Contributing
Feel free to open a pull request if you find new papers or code related to MoE and LLMs. Let's keep this list growing!
